# The MNIST Dataset
## Naive Bayes ##
After applying the Naive Bayes algorithm, we achieved an accuracy of 81.8% on the MNIST dataset. This indicates that the algorithm performs well in classifying the handwritten digits. Although, we observed that changing the laplace smoothing values can change the accuracy dramatically. Specifically, increasing the lapalace smoothing will decrease the accuracy as is shown in the graph.
The `NaiveBayes` class in `naive_bayes.py` contains a `predict` method, which takes the images, class probabilities, and pixel probabilities as input. It predicts the labels for the given images based on these probabilities. For each image, we calculate the probability of each label by summing the logarithm of the pixel probabilities multiplied by the corresponding pixel values in the image, and adding the logarithm of the class probability. We select the label with the highest probability as the predicted label for the image.
## Logistic Regression ##
After training the logistic regression model, we achieved an accuracy of 90.7% on the MNIST dataset. The model demonstrates a significant improvement over Naive Bayes, indicating its effectiveness in handling the classification task.
The `LogisticRegression` class in `logistic_regression.py` contains methods for initialization, softmax activation, and training the model. During initialization, the weights (`w`) and biases (`b`) are randomly initialized using a Gaussian distribution with mean 0 and standard deviation inversely proportional to the square root of the product of the number of inputs and classes.
The `softmax` function calculates the probabilities for each class using the softmax activation, ensuring that the probabilities sum up to 1. The `train` method trains the logistic regression model using gradient descent. It iterates over multiple epochs and mini-batches of the training data. The gradients of the weights and biases are calculated using the cross-entropy loss and updated using the momentum-based gradient descent with a learning rate.
